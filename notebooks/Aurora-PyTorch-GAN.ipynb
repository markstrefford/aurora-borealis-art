{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Aurora Borealis Art using GANs and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/pytorch/README.rst and https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::019326146125:role/service-role/AmazonSageMaker-ExecutionRole-20190407T165524\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='../train-sagemaker.py',\n",
    "                            role=role,\n",
    "                            train_instance_type='ml.p2.xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            framework_version='1.0.0',\n",
    "                            hyperparameters = {'epochs': 50,\n",
    "                                              'output_path': 's3://markstrefford-art-1/aurora-model-1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now set up training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2019-04-10-17-45-17-716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 17:45:18 Starting - Starting the training job...\n",
      "2019-04-10 17:45:19 Starting - Launching requested ML instances......\n",
      "2019-04-10 17:46:20 Starting - Preparing the instances for training......\n",
      "2019-04-10 17:47:37 Downloading - Downloading input data\n",
      "2019-04-10 17:47:37 Training - Downloading the training image......\n",
      "2019-04-10 17:48:42 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:44,332 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:44,359 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:44,965 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:45,263 sagemaker-containers INFO     Module train-sagemaker does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:45,264 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:45,264 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:45,264 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train-sagemaker\n",
      "  Running setup.py bdist_wheel for train-sagemaker: started\n",
      "  Running setup.py bdist_wheel for train-sagemaker: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dnj3r9ih/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train-sagemaker\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train-sagemaker\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-sagemaker-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.0.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-04-10 17:48:47,278 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"output_path\": \"s3://markstrefford-art-1/aurora-model-1\",\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-04-10-17-45-17-716\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-17-45-17-716/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-sagemaker\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-sagemaker.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":2,\"output_path\":\"s3://markstrefford-art-1/aurora-model-1\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train-sagemaker.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train-sagemaker\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-17-45-17-716/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":2,\"output_path\":\"s3://markstrefford-art-1/aurora-model-1\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-04-10-17-45-17-716\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-17-45-17-716/source/sourcedir.tar.gz\",\"module_name\":\"train-sagemaker\",\"network_interface_name\":\"ethwe\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train-sagemaker.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"2\",\"--output_path\",\"s3://markstrefford-art-1/aurora-model-1\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_OUTPUT_PATH=s3://markstrefford-art-1/aurora-model-1\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train-sagemaker --epochs 2 --output_path s3://markstrefford-art-1/aurora-model-1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31margs: Namespace(batch_size=64, beta1=0.5, data_dir='/opt/ml/input/data/training', epochs=2, lr=0.0002, manual_seed=999, model_dir='/opt/ml/model', output_data_dir='/opt/ml/output/data', use_cuda=True)\u001b[0m\n",
      "\u001b[31mdataroot=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mGenerator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\u001b[0m\n",
      "\u001b[31m)\u001b[0m\n",
      "\u001b[31mDiscriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\u001b[0m\n",
      "\u001b[31m)\u001b[0m\n",
      "\u001b[31mStarting Training Loop...\u001b[0m\n",
      "\u001b[31m[0/2][0/4]#011Loss_D: 1.7447#011Loss_G: 5.3661#011D(x): 0.5346#011D(G(z)): 0.5669 / 0.0069\u001b[0m\n",
      "\u001b[31m[1/2][0/4]#011Loss_D: 0.2413#011Loss_G: 6.6775#011D(x): 0.9339#011D(G(z)): 0.1361 / 0.0017\u001b[0m\n",
      "\u001b[31m2019-04-10 17:49:00,099 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-04-10 17:49:08 Uploading - Uploading generated training model\n",
      "2019-04-10 17:49:08 Completed - Training job completed\n",
      "Billable seconds: 99\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit({'training': 's3://markstrefford-art-1/aurora-art-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From examples... \n",
    "\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_lstm_word_language_model/pytorch_rnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-2-raw-v1.zip\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-04-10 15:38:15--  http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)... 52.216.17.48\n",
      "Connecting to research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)|52.216.17.48|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip.1’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  354K 13s\n",
      "    50K .......... .......... .......... .......... ..........  2% 1.38M 8s\n",
      "   100K .......... .......... .......... .......... ..........  3% 1.37M 6s\n",
      "   150K .......... .......... .......... .......... ..........  4% 60.2M 5s\n",
      "   200K .......... .......... .......... .......... ..........  5%  713K 5s\n",
      "   250K .......... .......... .......... .......... ..........  6% 90.4M 4s\n",
      "   300K .......... .......... .......... .......... ..........  7%  191M 3s\n",
      "   350K .......... .......... .......... .......... ..........  8% 1.39M 3s\n",
      "   400K .......... .......... .......... .......... ..........  9%  116M 3s\n",
      "   450K .......... .......... .......... .......... .......... 10% 1.42M 3s\n",
      "   500K .......... .......... .......... .......... .......... 11% 79.7M 3s\n",
      "   550K .......... .......... .......... .......... .......... 13% 97.9M 2s\n",
      "   600K .......... .......... .......... .......... .......... 14%  163M 2s\n",
      "   650K .......... .......... .......... .......... .......... 15%  272M 2s\n",
      "   700K .......... .......... .......... .......... .......... 16% 1.44M 2s\n",
      "   750K .......... .......... .......... .......... .......... 17%  151M 2s\n",
      "   800K .......... .......... .......... .......... .......... 18% 94.7M 2s\n",
      "   850K .......... .......... .......... .......... .......... 19%  151M 2s\n",
      "   900K .......... .......... .......... .......... .......... 20% 1.44M 2s\n",
      "   950K .......... .......... .......... .......... .......... 21% 99.8M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 22% 89.7M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 23%  116M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 24% 39.3M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 26%  107M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 27%  147M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 28%  100M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 29%  123M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 30%  127M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 31%  123M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 32% 1.60M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 33% 99.7M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 34%  102M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 35%  105M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 36%  130M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 37%  106M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 39%  138M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 40%  141M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 41% 1.49M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 42%  132M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 43%  126M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 44%  138M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 45% 89.0M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 46%  142M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 47% 97.3M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 48%  141M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 49%  125M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 50%  704K 1s\n",
      "  2350K .......... .......... .......... .......... .......... 52% 94.5M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 53%  384M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 54%  238M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 55% 93.6M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 56%  103M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 57%  194M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 58%  419M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 59%  332M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 60%  383M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 61%  313M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 62%  239M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 63% 1.73M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 65%  311M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 66%  380M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 67%  341M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 68%  373M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 69%  336M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 70%  353M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 71%  389M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 72%  305M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 73%  323M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 74%  383M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 75%  413M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 76%  370M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 78%  335M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 79%  393M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 80%  381M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 81%  389M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 82%  343M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 83%  378M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 84% 1.11M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 85% 96.5M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 86%  131M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 87% 38.8M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 88%  135M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 90%  136M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 91%  117M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 92%  113M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 93% 2.35M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 94%  122M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 95%  109M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 96%  136M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 97%  119M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 98% 1.16M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 99%  106M 0s\n",
      "  4600K ..........                                            100%  303M=0.7s\n",
      "\n",
      "2019-04-10 15:38:16 (6.28 MB/s) - ‘wikitext-2-raw-v1.zip.1’ saved [4721645/4721645]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
    "unzip -n wikitext-2-raw-v1.zip\n",
    "cd wikitext-2-raw\n",
    "mv wiki.test.raw test && mv wiki.train.raw train && mv wiki.valid.raw valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \r\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \r\n"
     ]
    }
   ],
   "source": [
    "!head -5 wikitext-2-raw/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-eu-west-1-019326146125/sagemaker/DEMO-pytorch-rnn-lstm\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-rnn-lstm'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='wikitext-2-raw', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.0.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    source_dir='../source',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'tied': True\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2019-04-10-15-38-24-889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 15:38:25 Starting - Starting the training job...\n",
      "2019-04-10 15:38:26 Starting - Launching requested ML instances.........\n",
      "2019-04-10 15:40:00 Starting - Preparing the instances for training....\n",
      "2019-04-10 15:41:21 Downloading - Downloading input data\n",
      "2019-04-10 15:41:21 Training - Downloading the training image......\n",
      "2019-04-10 15:42:27 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:29,045 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:29,071 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:30,477 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:30,775 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:30,775 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:30,775 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:30,776 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cvluuouf/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.0.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-04-10 15:42:32,715 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-04-10-15-38-24-889\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-15-38-24-889/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":6,\"tied\":true}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-15-38-24-889/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":6,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-04-10-15-38-24-889\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-15-38-24-889/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"ethwe\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"6\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 6 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mNamespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=6, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\u001b[0m\n",
      "\u001b[31mLoad data\u001b[0m\n",
      "\u001b[31mBatchify dataset\u001b[0m\n",
      "\u001b[31mBuild the model\u001b[0m\n",
      "\u001b[31mStarting training.\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 92.67 | loss  8.33 | ppl  4130.03\u001b[0m\n",
      "\u001b[31m| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 94.49 | loss  7.38 | ppl  1607.70\u001b[0m\n",
      "\u001b[31m| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 93.86 | loss  6.88 | ppl   968.84\u001b[0m\n",
      "\u001b[31m| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 94.47 | loss  6.68 | ppl   796.87\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 93.47 | loss  6.45 | ppl   634.33\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 91.51 | loss  6.39 | ppl   595.52\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 91.73 | loss  6.32 | ppl   553.12\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 93.19 | loss  5.10 | ppl   164.07\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 94.00 | loss  5.04 | ppl   154.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 290.67s | valid loss  5.59 | valid ppl   267.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 4, 'lr': 20, 'val_loss': 5.587578046959433, 'val_ppl': 267.0879612312184}\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 93.49 | loss  5.11 | ppl   165.10\u001b[0m\n",
      "\u001b[31m| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 93.49 | loss  5.12 | ppl   167.90\u001b[0m\n",
      "\u001b[31m| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 92.68 | loss  4.93 | ppl   138.36\u001b[0m\n",
      "\u001b[31m| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 94.14 | loss  4.99 | ppl   147.39\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.24 | loss  4.94 | ppl   140.26\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.66 | loss  4.98 | ppl   145.48\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 93.19 | loss  5.06 | ppl   156.83\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.05 | loss  5.13 | ppl   168.43\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
