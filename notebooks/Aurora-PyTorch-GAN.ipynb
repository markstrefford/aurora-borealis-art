{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Aurora Borealis Art using GANs and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/pytorch/README.rst and https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::019326146125:role/service-role/AmazonSageMaker-ExecutionRole-20190407T165524\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point='../train-sagemaker.py',\n",
    "                            role=role,\n",
    "                            train_instance_type='ml.p2.xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            framework_version='1.0.0',\n",
    "                            output_path='s3://markstrefford-art-1/aurora-model-1',\n",
    "                            hyperparameters = {'epochs': 50 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now set up training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2019-04-10-20-48-23-821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 20:48:24 Starting - Starting the training job...\n",
      "2019-04-10 20:48:25 Starting - Launching requested ML instances......\n",
      "2019-04-10 20:49:28 Starting - Preparing the instances for training.........\n",
      "2019-04-10 20:51:16 Downloading - Downloading input data...\n",
      "2019-04-10 20:51:29 Training - Downloading the training image...\n",
      "2019-04-10 20:52:13 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,001 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,029 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,634 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,875 sagemaker-containers INFO     Module train-sagemaker does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,876 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,876 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:15,876 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train-sagemaker\n",
      "  Running setup.py bdist_wheel for train-sagemaker: started\n",
      "  Running setup.py bdist_wheel for train-sagemaker: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-85k50ypd/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train-sagemaker\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train-sagemaker\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-sagemaker-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.0.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-04-10 20:52:17,790 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 50\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-04-10-20-48-23-821\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://markstrefford-art-1/sagemaker-pytorch-2019-04-10-20-48-23-821/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-sagemaker\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-sagemaker.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":50}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train-sagemaker.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train-sagemaker\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://markstrefford-art-1/sagemaker-pytorch-2019-04-10-20-48-23-821/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":50},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-04-10-20-48-23-821\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://markstrefford-art-1/sagemaker-pytorch-2019-04-10-20-48-23-821/source/sourcedir.tar.gz\",\"module_name\":\"train-sagemaker\",\"network_interface_name\":\"ethwe\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train-sagemaker.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"50\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train-sagemaker --epochs 50\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31margs: Namespace(batch_size=64, beta1=0.5, data_dir='/opt/ml/input/data/training', epochs=50, lr=0.0002, manual_seed=999, model_dir='/opt/ml/model', output_data_dir='/opt/ml/output/data', use_cuda=True)\u001b[0m\n",
      "\u001b[31mdataroot=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mGenerator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\u001b[0m\n",
      "\u001b[31m)\u001b[0m\n",
      "\u001b[31mDiscriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\u001b[0m\n",
      "\u001b[31m)\u001b[0m\n",
      "\u001b[31mStarting Training Loop...\u001b[0m\n",
      "\u001b[31m[0/50][0/4]#011Loss_D: 1.7447#011Loss_G: 5.3661#011D(x): 0.5346#011D(G(z)): 0.5669 / 0.0069\u001b[0m\n",
      "\u001b[31m[1/50][0/4]#011Loss_D: 0.2461#011Loss_G: 6.7241#011D(x): 0.9332#011D(G(z)): 0.1386 / 0.0016\u001b[0m\n",
      "\u001b[31m[2/50][0/4]#011Loss_D: 0.1810#011Loss_G: 8.4994#011D(x): 0.9532#011D(G(z)): 0.1088 / 0.0003\u001b[0m\n",
      "\u001b[31m[3/50][0/4]#011Loss_D: 0.0982#011Loss_G: 8.4531#011D(x): 0.9774#011D(G(z)): 0.0693 / 0.0003\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[4/50][0/4]#011Loss_D: 0.0720#011Loss_G: 9.0210#011D(x): 0.9427#011D(G(z)): 0.0060 / 0.0002\u001b[0m\n",
      "\u001b[31m[5/50][0/4]#011Loss_D: 0.1463#011Loss_G: 16.3454#011D(x): 0.8917#011D(G(z)): 0.0000 / 0.0000\u001b[0m\n",
      "\u001b[31m[6/50][0/4]#011Loss_D: 0.0014#011Loss_G: 8.8170#011D(x): 0.9988#011D(G(z)): 0.0001 / 0.0003\u001b[0m\n",
      "\u001b[31m[7/50][0/4]#011Loss_D: 0.0213#011Loss_G: 8.8217#011D(x): 0.9814#011D(G(z)): 0.0006 / 0.0002\u001b[0m\n",
      "\u001b[31m[8/50][0/4]#011Loss_D: 0.0078#011Loss_G: 10.5726#011D(x): 0.9925#011D(G(z)): 0.0001 / 0.0000\u001b[0m\n",
      "\u001b[31m[9/50][0/4]#011Loss_D: 0.8030#011Loss_G: 20.3478#011D(x): 0.9734#011D(G(z)): 0.4963 / 0.0000\u001b[0m\n",
      "\u001b[31m[10/50][0/4]#011Loss_D: 2.5559#011Loss_G: 25.6901#011D(x): 0.9963#011D(G(z)): 0.8843 / 0.0000\u001b[0m\n",
      "\u001b[31m[11/50][0/4]#011Loss_D: 0.0052#011Loss_G: 27.0071#011D(x): 0.9949#011D(G(z)): 0.0000 / 0.0000\u001b[0m\n",
      "\u001b[31m[12/50][0/4]#011Loss_D: 1.4850#011Loss_G: 25.0927#011D(x): 0.9984#011D(G(z)): 0.7301 / 0.0000\u001b[0m\n",
      "\u001b[31m[13/50][0/4]#011Loss_D: 0.0090#011Loss_G: 26.4901#011D(x): 0.9914#011D(G(z)): 0.0000 / 0.0000\u001b[0m\n",
      "\u001b[31m[14/50][0/4]#011Loss_D: 0.7133#011Loss_G: 19.7126#011D(x): 0.9969#011D(G(z)): 0.4375 / 0.0000\u001b[0m\n",
      "\u001b[31m[15/50][0/4]#011Loss_D: 0.1030#011Loss_G: 6.4375#011D(x): 0.9784#011D(G(z)): 0.0745 / 0.0021\u001b[0m\n",
      "\u001b[31m[16/50][0/4]#011Loss_D: 0.0194#011Loss_G: 7.7885#011D(x): 0.9823#011D(G(z)): 0.0002 / 0.0005\u001b[0m\n",
      "\u001b[31m[17/50][0/4]#011Loss_D: 0.0435#011Loss_G: 5.6215#011D(x): 0.9914#011D(G(z)): 0.0336 / 0.0046\u001b[0m\n",
      "\u001b[31m[18/50][0/4]#011Loss_D: 0.3338#011Loss_G: 10.7366#011D(x): 0.9961#011D(G(z)): 0.2619 / 0.0000\u001b[0m\n",
      "\u001b[31m[19/50][0/4]#011Loss_D: 1.2422#011Loss_G: 10.0897#011D(x): 0.5070#011D(G(z)): 0.0000 / 0.0001\u001b[0m\n",
      "\u001b[31m[20/50][0/4]#011Loss_D: 0.0362#011Loss_G: 8.7140#011D(x): 0.9781#011D(G(z)): 0.0081 / 0.0048\u001b[0m\n",
      "\u001b[31m[21/50][0/4]#011Loss_D: 0.8493#011Loss_G: 17.8752#011D(x): 0.9534#011D(G(z)): 0.4714 / 0.0000\u001b[0m\n",
      "\u001b[31m[22/50][0/4]#011Loss_D: 0.3455#011Loss_G: 7.0290#011D(x): 0.8631#011D(G(z)): 0.1042 / 0.0017\u001b[0m\n",
      "\u001b[31m[23/50][0/4]#011Loss_D: 3.7428#011Loss_G: 4.9945#011D(x): 0.2747#011D(G(z)): 0.0125 / 0.0670\u001b[0m\n",
      "\u001b[31m[24/50][0/4]#011Loss_D: 0.6447#011Loss_G: 5.2831#011D(x): 0.8734#011D(G(z)): 0.3435 / 0.0095\u001b[0m\n",
      "\u001b[31m[25/50][0/4]#011Loss_D: 0.8622#011Loss_G: 4.9202#011D(x): 0.9715#011D(G(z)): 0.5151 / 0.0107\u001b[0m\n",
      "\u001b[31m[26/50][0/4]#011Loss_D: 0.5548#011Loss_G: 4.1716#011D(x): 0.8350#011D(G(z)): 0.2455 / 0.0187\u001b[0m\n",
      "\u001b[31m[27/50][0/4]#011Loss_D: 0.4650#011Loss_G: 3.8599#011D(x): 0.8447#011D(G(z)): 0.2057 / 0.0319\u001b[0m\n",
      "\u001b[31m[28/50][0/4]#011Loss_D: 0.6008#011Loss_G: 2.9701#011D(x): 0.9623#011D(G(z)): 0.3579 / 0.0898\u001b[0m\n",
      "\u001b[31m[29/50][0/4]#011Loss_D: 0.3341#011Loss_G: 3.8814#011D(x): 0.9109#011D(G(z)): 0.1871 / 0.0277\u001b[0m\n",
      "\u001b[31m[30/50][0/4]#011Loss_D: 0.3839#011Loss_G: 4.3884#011D(x): 0.8909#011D(G(z)): 0.2171 / 0.0154\u001b[0m\n",
      "\u001b[31m[31/50][0/4]#011Loss_D: 0.3020#011Loss_G: 3.6808#011D(x): 0.8731#011D(G(z)): 0.1341 / 0.0336\u001b[0m\n",
      "\u001b[31m[32/50][0/4]#011Loss_D: 0.3285#011Loss_G: 4.9415#011D(x): 0.9347#011D(G(z)): 0.2208 / 0.0087\u001b[0m\n",
      "\u001b[31m[33/50][0/4]#011Loss_D: 0.1906#011Loss_G: 2.7894#011D(x): 0.9078#011D(G(z)): 0.0774 / 0.0718\u001b[0m\n",
      "\u001b[31m[34/50][0/4]#011Loss_D: 0.2408#011Loss_G: 5.0995#011D(x): 0.9468#011D(G(z)): 0.1638 / 0.0074\u001b[0m\n",
      "\u001b[31m[35/50][0/4]#011Loss_D: 0.2721#011Loss_G: 3.7320#011D(x): 0.9800#011D(G(z)): 0.2118 / 0.0288\u001b[0m\n",
      "\u001b[31m[36/50][0/4]#011Loss_D: 0.1979#011Loss_G: 4.4070#011D(x): 0.9420#011D(G(z)): 0.1227 / 0.0157\u001b[0m\n",
      "\u001b[31m[37/50][0/4]#011Loss_D: 0.4120#011Loss_G: 1.8687#011D(x): 0.7218#011D(G(z)): 0.0233 / 0.1826\u001b[0m\n",
      "\u001b[31m[38/50][0/4]#011Loss_D: 0.1683#011Loss_G: 9.7302#011D(x): 0.8618#011D(G(z)): 0.0013 / 0.0002\u001b[0m\n",
      "\u001b[31m[39/50][0/4]#011Loss_D: 0.1155#011Loss_G: 5.9313#011D(x): 0.9217#011D(G(z)): 0.0264 / 0.0055\u001b[0m\n",
      "\u001b[31m[40/50][0/4]#011Loss_D: 0.1374#011Loss_G: 5.1653#011D(x): 0.9567#011D(G(z)): 0.0830 / 0.0111\u001b[0m\n",
      "\u001b[31m[41/50][0/4]#011Loss_D: 2.4291#011Loss_G: 6.5651#011D(x): 0.1940#011D(G(z)): 0.0001 / 0.0092\u001b[0m\n",
      "\u001b[31m[42/50][0/4]#011Loss_D: 0.0964#011Loss_G: 3.2573#011D(x): 0.9204#011D(G(z)): 0.0078 / 0.0576\u001b[0m\n",
      "\u001b[31m[43/50][0/4]#011Loss_D: 0.5937#011Loss_G: 7.3037#011D(x): 0.9646#011D(G(z)): 0.3740 / 0.0014\u001b[0m\n",
      "\u001b[31m[44/50][0/4]#011Loss_D: 0.2108#011Loss_G: 7.4063#011D(x): 0.8285#011D(G(z)): 0.0075 / 0.0019\u001b[0m\n",
      "\u001b[31m[45/50][0/4]#011Loss_D: 0.0642#011Loss_G: 6.1733#011D(x): 0.9648#011D(G(z)): 0.0259 / 0.0029\u001b[0m\n",
      "\u001b[31m[46/50][0/4]#011Loss_D: 0.0922#011Loss_G: 5.5377#011D(x): 0.9475#011D(G(z)): 0.0317 / 0.0053\u001b[0m\n",
      "\u001b[31m[47/50][0/4]#011Loss_D: 0.2267#011Loss_G: 4.5887#011D(x): 0.8848#011D(G(z)): 0.0706 / 0.0125\u001b[0m\n",
      "\u001b[31m[48/50][0/4]#011Loss_D: 0.6837#011Loss_G: 12.7052#011D(x): 0.9724#011D(G(z)): 0.4448 / 0.0000\u001b[0m\n",
      "\n",
      "2019-04-10 20:54:21 Uploading - Uploading generated training model\n",
      "2019-04-10 20:54:21 Completed - Training job completed\n",
      "\u001b[31m[49/50][0/4]#011Loss_D: 0.0444#011Loss_G: 6.3026#011D(x): 0.9858#011D(G(z)): 0.0287 / 0.0025\u001b[0m\n",
      "\u001b[31mSaving final model: {'epoch': 22, 'lr': 0.0002, 'errD': 2.2183337211608887, 'errG': 0.00202673627063632, 'D_x': 0.2570493221282959, 'D_G_z1': 0.003962279763072729, 'D_G_z2': 0.997994065284729}\u001b[0m\n",
      "\u001b[31m2019-04-10 20:54:15,052 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Billable seconds: 185\n"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit({'training': 's3://markstrefford-art-1/aurora-art-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From examples... \n",
    "\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/pytorch_lstm_word_language_model/pytorch_rnn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-2-raw-v1.zip\n",
      "   creating: wikitext-2-raw/\n",
      "  inflating: wikitext-2-raw/wiki.test.raw  \n",
      "  inflating: wikitext-2-raw/wiki.valid.raw  \n",
      "  inflating: wikitext-2-raw/wiki.train.raw  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-04-10 19:38:34--  http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
      "Resolving research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)... 52.216.138.171\n",
      "Connecting to research.metamind.io.s3.amazonaws.com (research.metamind.io.s3.amazonaws.com)|52.216.138.171|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4721645 (4.5M) [application/zip]\n",
      "Saving to: ‘wikitext-2-raw-v1.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  353K 13s\n",
      "    50K .......... .......... .......... .......... ..........  2% 1.39M 8s\n",
      "   100K .......... .......... .......... .......... ..........  3% 1.35M 6s\n",
      "   150K .......... .......... .......... .......... ..........  4% 81.1M 5s\n",
      "   200K .......... .......... .......... .......... ..........  5%  712K 5s\n",
      "   250K .......... .......... .......... .......... ..........  6% 60.0M 4s\n",
      "   300K .......... .......... .......... .......... ..........  7% 75.8M 3s\n",
      "   350K .......... .......... .......... .......... ..........  8% 81.0M 3s\n",
      "   400K .......... .......... .......... .......... ..........  9% 1.48M 3s\n",
      "   450K .......... .......... .......... .......... .......... 10% 1.36M 3s\n",
      "   500K .......... .......... .......... .......... .......... 11% 92.6M 3s\n",
      "   550K .......... .......... .......... .......... .......... 13%  100M 2s\n",
      "   600K .......... .......... .......... .......... .......... 14% 72.6M 2s\n",
      "   650K .......... .......... .......... .......... .......... 15% 83.7M 2s\n",
      "   700K .......... .......... .......... .......... .......... 16%  377M 2s\n",
      "   750K .......... .......... .......... .......... .......... 17% 62.0M 2s\n",
      "   800K .......... .......... .......... .......... .......... 18% 89.3M 2s\n",
      "   850K .......... .......... .......... .......... .......... 19% 1.56M 2s\n",
      "   900K .......... .......... .......... .......... .......... 20% 1.37M 2s\n",
      "   950K .......... .......... .......... .......... .......... 21%  135M 2s\n",
      "  1000K .......... .......... .......... .......... .......... 22%  126M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 23%  123M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 24%  102M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 26% 83.4M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 27%  127M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 28%  110M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 29%  162M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 30% 93.7M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 31%  697K 1s\n",
      "  1450K .......... .......... .......... .......... .......... 32%  107M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 33% 90.6M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 34%  353M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 35%  386M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 36%  422M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 37%  333M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 39% 1.68M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 40%  158M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 41%  357M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 42%  378M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 43%  342M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 44%  336M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 45%  376M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 46%  350M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 47% 1.31M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 48%  125M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 49%  108M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 50%  193M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 52%  199M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 53%  136M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 54%  151M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 55%  205M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 56%  214M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 57% 87.0M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 58% 61.7M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 59% 93.5M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 60%  110M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 61%  177M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 62%  101M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 63%  367M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 65% 1.80M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 66% 1.29M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 67%  114M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 68%  124M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 69% 99.4M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 70%  102M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 71%  111M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 72% 86.0M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 73% 94.1M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 74% 97.3M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 75%  136M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 76%  124M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 78% 92.2M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 79%  131M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 80%  133M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 81% 92.9M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 82% 99.9M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 83% 92.8M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 84%  212M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 85%  114M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 86% 1.94M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 87% 1.30M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 88%  101M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 90%  126M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 91%  121M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 92% 98.0M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 93%  129M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 94%  105M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 95% 99.4M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 96% 74.7M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 97%  129M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 98%  112M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 99%  123M 0s\n",
      "  4600K ..........                                            100%  364M=0.7s\n",
      "\n",
      "2019-04-10 19:38:35 (6.29 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget http://research.metamind.io.s3.amazonaws.com/wikitext/wikitext-2-raw-v1.zip\n",
    "unzip -n wikitext-2-raw-v1.zip\n",
    "cd wikitext-2-raw\n",
    "mv wiki.test.raw test && mv wiki.train.raw train && mv wiki.valid.raw valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \r\n",
      " = Valkyria Chronicles III = \r\n",
      " \r\n",
      " Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \r\n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \r\n"
     ]
    }
   ],
   "source": [
    "!head -5 wikitext-2-raw/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-eu-west-1-019326146125/sagemaker/DEMO-pytorch-rnn-lstm\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-rnn-lstm'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='wikitext-2-raw', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.0.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    source_dir='../source',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'tied': True\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2019-04-10-19-38-41-369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10 19:38:42 Starting - Starting the training job...\n",
      "2019-04-10 19:38:44 Starting - Launching requested ML instances......\n",
      "2019-04-10 19:39:47 Starting - Preparing the instances for training.........\n",
      "2019-04-10 19:41:38 Downloading - Downloading input data\n",
      "2019-04-10 19:41:38 Training - Downloading the training image.....\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,029 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,056 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,262 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,579 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,580 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,580 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:18,580 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-goj0o3po/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.0.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-04-10 19:42:21,082 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"tied\": true,\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-04-10-19-38-41-369\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-19-38-41-369/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":6,\"tied\":true}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-19-38-41-369/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":6,\"tied\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-04-10-19-38-41-369\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-019326146125/sagemaker-pytorch-2019-04-10-19-38-41-369/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"ethwe\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"6\",\"--tied\",\"True\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_TIED=true\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train --epochs 6 --tied True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mNamespace(batch_size=20, bptt=35, clip=0.25, data_dir='/opt/ml/input/data/training', dropout=0.2, emsize=200, epochs=6, log_interval=200, lr=20, model_dir='/opt/ml/model', nhid=200, nlayers=2, output_data_dir='/opt/ml/output/data', seed=1111, tied=True)\u001b[0m\n",
      "\u001b[31mLoad data\u001b[0m\n",
      "\n",
      "2019-04-10 19:42:15 Training - Training image download completed. Training in progress.\u001b[31mBatchify dataset\u001b[0m\n",
      "\u001b[31mBuild the model\u001b[0m\n",
      "\u001b[31mStarting training.\u001b[0m\n",
      "\u001b[31m| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 90.22 | loss  8.33 | ppl  4130.03\u001b[0m\n",
      "\u001b[31m| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 90.24 | loss  7.38 | ppl  1607.70\u001b[0m\n",
      "\u001b[31m| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 90.60 | loss  6.88 | ppl   968.84\u001b[0m\n",
      "\u001b[31m| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 90.96 | loss  6.68 | ppl   796.87\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 91.25 | loss  6.45 | ppl   634.33\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 91.26 | loss  6.39 | ppl   595.52\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 91.42 | loss  6.32 | ppl   553.12\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 91.18 | loss  6.29 | ppl   541.34\u001b[0m\n",
      "\u001b[31m| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 91.41 | loss  6.08 | ppl   435.14\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 91.44 | loss  6.04 | ppl   421.86\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 91.63 | loss  5.93 | ppl   376.03\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 91.77 | loss  5.97 | ppl   389.69\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 91.64 | loss  5.94 | ppl   379.73\u001b[0m\n",
      "\u001b[31m| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 91.87 | loss  5.83 | ppl   340.14\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   1 | time: 282.02s | valid loss  6.01 | valid ppl   407.35\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 1, 'lr': 20, 'val_loss': 6.009682313525223, 'val_ppl': 407.3538888947412}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 92.21 | loss  5.83 | ppl   341.13\u001b[0m\n",
      "\u001b[31m| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 92.03 | loss  5.81 | ppl   332.28\u001b[0m\n",
      "\u001b[31m| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 92.09 | loss  5.60 | ppl   270.83\u001b[0m\n",
      "\u001b[31m| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 92.05 | loss  5.62 | ppl   277.03\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.15 | loss  5.52 | ppl   250.46\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.04 | loss  5.55 | ppl   256.81\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.02 | loss  5.60 | ppl   269.26\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.06 | loss  5.65 | ppl   285.00\u001b[0m\n",
      "\u001b[31m| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.16 | loss  5.48 | ppl   240.43\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.17 | loss  5.49 | ppl   242.47\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.10 | loss  5.39 | ppl   219.57\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.03 | loss  5.45 | ppl   233.69\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 91.98 | loss  5.46 | ppl   235.99\u001b[0m\n",
      "\u001b[31m| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 91.97 | loss  5.39 | ppl   218.22\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   2 | time: 284.46s | valid loss  5.75 | valid ppl   312.71\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 2, 'lr': 20, 'val_loss': 5.745290185900826, 'val_ppl': 312.71435996307196}\u001b[0m\n",
      "\u001b[31m| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 92.41 | loss  5.44 | ppl   231.04\u001b[0m\n",
      "\u001b[31m| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 92.07 | loss  5.45 | ppl   231.83\u001b[0m\n",
      "\u001b[31m| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 92.26 | loss  5.24 | ppl   189.53\u001b[0m\n",
      "\u001b[31m| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 92.21 | loss  5.30 | ppl   200.62\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.18 | loss  5.22 | ppl   184.66\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.19 | loss  5.26 | ppl   192.02\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.14 | loss  5.32 | ppl   205.14\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.07 | loss  5.39 | ppl   219.50\u001b[0m\n",
      "\u001b[31m| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.20 | loss  5.22 | ppl   185.64\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.25 | loss  5.26 | ppl   192.65\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.21 | loss  5.15 | ppl   172.36\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.29 | loss  5.22 | ppl   185.43\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.30 | loss  5.25 | ppl   189.63\u001b[0m\n",
      "\u001b[31m| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.24 | loss  5.17 | ppl   176.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   3 | time: 284.87s | valid loss  5.65 | valid ppl   282.88\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 3, 'lr': 20, 'val_loss': 5.64503262378652, 'val_ppl': 282.88278477982595}\u001b[0m\n",
      "\u001b[31m| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 92.73 | loss  5.24 | ppl   188.27\u001b[0m\n",
      "\u001b[31m| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 92.19 | loss  5.26 | ppl   192.07\u001b[0m\n",
      "\u001b[31m| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 92.56 | loss  5.05 | ppl   156.44\u001b[0m\n",
      "\u001b[31m| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 92.47 | loss  5.12 | ppl   167.54\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.42 | loss  5.05 | ppl   156.79\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.57 | loss  5.09 | ppl   162.70\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.52 | loss  5.17 | ppl   175.82\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.52 | loss  5.23 | ppl   187.09\u001b[0m\n",
      "\u001b[31m| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.60 | loss  5.08 | ppl   160.19\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.56 | loss  5.11 | ppl   166.44\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.57 | loss  5.01 | ppl   149.18\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.57 | loss  5.08 | ppl   160.23\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.63 | loss  5.10 | ppl   164.07\u001b[0m\n",
      "\u001b[31m| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.63 | loss  5.04 | ppl   154.60\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   4 | time: 285.88s | valid loss  5.59 | valid ppl   267.09\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 4, 'lr': 20, 'val_loss': 5.587578046959433, 'val_ppl': 267.0879612312184}\u001b[0m\n",
      "\u001b[31m| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 93.08 | loss  5.11 | ppl   165.10\u001b[0m\n",
      "\u001b[31m| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 92.60 | loss  5.12 | ppl   167.90\u001b[0m\n",
      "\u001b[31m| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 92.65 | loss  4.93 | ppl   138.36\u001b[0m\n",
      "\u001b[31m| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 92.61 | loss  4.99 | ppl   147.39\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.57 | loss  4.94 | ppl   140.26\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.69 | loss  4.98 | ppl   145.48\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.66 | loss  5.06 | ppl   156.83\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.68 | loss  5.13 | ppl   168.43\u001b[0m\n",
      "\u001b[31m| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.67 | loss  4.97 | ppl   143.89\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.72 | loss  5.01 | ppl   149.88\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.80 | loss  4.91 | ppl   135.04\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.78 | loss  4.98 | ppl   144.97\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.68 | loss  5.00 | ppl   148.20\u001b[0m\n",
      "\u001b[31m| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.72 | loss  4.94 | ppl   140.39\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   5 | time: 286.35s | valid loss  5.58 | valid ppl   265.55\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 5, 'lr': 20, 'val_loss': 5.58179480909885, 'val_ppl': 265.547785914524}\u001b[0m\n",
      "\u001b[31m| epoch   6 |   200/ 2983 batches | lr 20.00 | ms/batch 93.11 | loss  5.01 | ppl   149.77\u001b[0m\n",
      "\u001b[31m| epoch   6 |   400/ 2983 batches | lr 20.00 | ms/batch 92.67 | loss  5.02 | ppl   151.70\u001b[0m\n",
      "\u001b[31m| epoch   6 |   600/ 2983 batches | lr 20.00 | ms/batch 92.70 | loss  4.83 | ppl   125.01\u001b[0m\n",
      "\u001b[31m| epoch   6 |   800/ 2983 batches | lr 20.00 | ms/batch 92.75 | loss  4.90 | ppl   134.81\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1000/ 2983 batches | lr 20.00 | ms/batch 92.75 | loss  4.86 | ppl   128.49\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1200/ 2983 batches | lr 20.00 | ms/batch 92.69 | loss  4.89 | ppl   133.32\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1400/ 2983 batches | lr 20.00 | ms/batch 92.73 | loss  4.97 | ppl   144.30\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1600/ 2983 batches | lr 20.00 | ms/batch 92.63 | loss  5.04 | ppl   153.77\u001b[0m\n",
      "\u001b[31m| epoch   6 |  1800/ 2983 batches | lr 20.00 | ms/batch 92.77 | loss  4.89 | ppl   133.15\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2000/ 2983 batches | lr 20.00 | ms/batch 92.76 | loss  4.93 | ppl   138.80\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2200/ 2983 batches | lr 20.00 | ms/batch 92.76 | loss  4.82 | ppl   123.97\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m| epoch   6 |  2400/ 2983 batches | lr 20.00 | ms/batch 92.76 | loss  4.89 | ppl   133.57\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2600/ 2983 batches | lr 20.00 | ms/batch 92.84 | loss  4.92 | ppl   137.33\u001b[0m\n",
      "\u001b[31m| epoch   6 |  2800/ 2983 batches | lr 20.00 | ms/batch 92.77 | loss  4.87 | ppl   129.87\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31m| end of epoch   6 | time: 286.53s | valid loss  5.55 | valid ppl   258.43\u001b[0m\n",
      "\u001b[31m-----------------------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[31mSaving the best model: {'epoch': 6, 'lr': 20, 'val_loss': 5.554613220146136, 'val_ppl': 258.4269908341668}\u001b[0m\n",
      "\n",
      "2019-04-10 20:11:28 Uploading - Uploading generated training model\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m| End of training | test loss  5.57 | test ppl   261.20\u001b[0m\n",
      "\u001b[31m=========================================================================================\u001b[0m\n",
      "\u001b[31m2019-04-10 20:11:25,646 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-04-10 20:11:49 Completed - Training job completed\n",
      "Billable seconds: 1828\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
